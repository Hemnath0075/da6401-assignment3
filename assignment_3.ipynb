{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a39238",
   "metadata": {},
   "source": [
    "## Question 1 (15 Marks)\n",
    "\n",
    "Build a RNN based seq2seq model which contains the following layers: (i) input layer for character embeddings (ii) one encoder RNN which sequentially encodes the input character sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time (Devanagari).\n",
    "\n",
    "The code should be flexible such that the dimension of the input character embeddings, the hidden states of the encoders and decoders, the cell (RNN, LSTM, GRU) and the number of layers in the encoder and decoder can be changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33f4e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, embed_size, hidden_size, num_layers=1, cell_type=\"LSTM\"):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
    "        rnn_class = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell_type]\n",
    "        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_vocab_size, embed_size, hidden_size, num_layers=1, cell_type=\"LSTM\"):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
    "        rnn_class = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell_type]\n",
    "        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        predictions = self.fc(output.squeeze(1))  # (batch_size, vocab_size)\n",
    "        return predictions, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, cell_type=\"LSTM\"):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size, target_len = target.size()\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(device)\n",
    "        hidden = self.encoder(source)\n",
    "\n",
    "        input = target[:, 0].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30e415",
   "metadata": {},
   "source": [
    "(a) What is the total number of computations done by your network? (assume that the input embedding size is m, encoder and decoder have 1 layer each, the hidden cell state is kkk for both the encoder and decoder, the length of the input and output sequence is the same, i.e., T, the size of the vocabulary is the same for the source and target language, i.e., V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2bb95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d2dd19c",
   "metadata": {},
   "source": [
    "(b) What is the total number of parameters in your network? (assume that the input embedding size is M, encoder and decoder have 1 layer each, the hidden cell state is k for both the encoder and decoder and the length of the input and output sequence is the same, i.e., T, the size of the vocabulary is the same for the source and target language, i.e., V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d9de3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8532c72",
   "metadata": {},
   "source": [
    "## Question 2 (10 Marks)\n",
    "You will now train your model using any one language from the Dakshina dataset (I would suggest pick a language that you can read so that it is easy to analyse the errors). Use the standard train, dev, test set from the folder dakshina_dataset_v1.0/hi/lexicons/ (replace hi by the language of your choice)\n",
    "\n",
    "Using the sweep feature in wandb find the best hyperparameter configuration. Here are some suggestions but you are free to decide which hyperparameters you want to explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fd08831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filepaths):\n",
    "    chars = set()\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                native, roman, _ = line.strip().split(\"\\t\")\n",
    "                chars.update(native)\n",
    "                chars.update(roman)\n",
    "    return chars\n",
    "\n",
    "def make_char2idx(char_set):\n",
    "    char_list = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + sorted(list(char_set))\n",
    "    return {char: idx for idx, char in enumerate(char_list)}, char_list\n",
    "\n",
    "train_path = \"/mnt/e_disk/ch24s016/da6401_assignment3/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\"\n",
    "dev_path = \"/mnt/e_disk/ch24s016/da6401_assignment3/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\"\n",
    "\n",
    "char_set = build_vocab([train_path, dev_path])\n",
    "roman2idx, idx2roman = make_char2idx(set(c for c in char_set if c.isascii()))\n",
    "devanagari2idx, idx2devanagari = make_char2idx(set(c for c in char_set if not c.isascii()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2844381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=roman2idx[\"<pad>\"], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=devanagari2idx[\"<pad>\"], batch_first=True)\n",
    "    return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf0e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, tsv_path, src_char2idx, tgt_char2idx, max_len=32):\n",
    "        self.pairs = []\n",
    "        with open(tsv_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                native, roman, _ = line.strip().split('\\t')\n",
    "                self.pairs.append((roman, native))\n",
    "\n",
    "        self.src_c2i = src_char2idx\n",
    "        self.tgt_c2i = tgt_char2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        roman, native = self.pairs[i]\n",
    "\n",
    "        # map chars â†’ indices, add <sos> / <eos> tokens as needed\n",
    "        src_idxs = [self.src_c2i.get(c, self.src_c2i[\"<unk>\"]) \n",
    "                    for c in roman][: self.max_len]\n",
    "        tgt_idxs = [self.tgt_c2i[\"<sos>\"]] + \\\n",
    "                   [self.tgt_c2i.get(c, self.tgt_c2i[\"<unk>\"]) \n",
    "                    for c in native][: (self.max_len-1)] + \\\n",
    "                   [self.tgt_c2i[\"<eos>\"]]\n",
    "\n",
    "        return torch.tensor(src_idxs), torch.tensor(tgt_idxs)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "\n",
    "    src_max_len = max(seq.size(0) for seq in src_seqs)\n",
    "    tgt_max_len = max(seq.size(0) for seq in tgt_seqs)\n",
    "\n",
    "\n",
    "    src_padded = torch.stack([\n",
    "        torch.cat([seq, torch.full((src_max_len - len(seq),), roman2idx[\"<pad>\"], dtype=torch.long)])\n",
    "        for seq in src_seqs\n",
    "    ])\n",
    "\n",
    "    tgt_padded = torch.stack([\n",
    "        torch.cat([seq, torch.full((tgt_max_len - len(seq),), devanagari2idx[\"<pad>\"], dtype=torch.long)])\n",
    "        for seq in tgt_seqs\n",
    "    ])\n",
    "\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "train_ds = TransliterationDataset(\n",
    "    \"/mnt/e_disk/ch24s016/da6401_assignment3/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\",\n",
    "    src_char2idx=roman2idx,\n",
    "    tgt_char2idx=devanagari2idx,\n",
    "    max_len=32\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "train_dataset = TransliterationDataset(train_path, roman2idx, devanagari2idx, max_len=32)\n",
    "dev_dataset = TransliterationDataset(dev_path, roman2idx, devanagari2idx, max_len=32)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421154af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 3127.1562, Train Acc: 0.4816, Dev Acc: 0.4805\n",
      "Epoch 2/10, Loss: 1711.6280, Train Acc: 0.5946, Dev Acc: 0.5804\n",
      "Epoch 3/10, Loss: 1352.0774, Train Acc: 0.6470, Dev Acc: 0.6213\n",
      "Epoch 4/10, Loss: 1176.7243, Train Acc: 0.6764, Dev Acc: 0.6408\n",
      "Epoch 5/10, Loss: 1063.9765, Train Acc: 0.7036, Dev Acc: 0.6505\n",
      "Epoch 6/10, Loss: 979.7534, Train Acc: 0.7207, Dev Acc: 0.6639\n",
      "Epoch 7/10, Loss: 919.6203, Train Acc: 0.7417, Dev Acc: 0.6652\n",
      "Epoch 8/10, Loss: 864.9013, Train Acc: 0.7526, Dev Acc: 0.6724\n",
      "Epoch 9/10, Loss: 820.9855, Train Acc: 0.7637, Dev Acc: 0.6734\n",
      "Epoch 10/10, Loss: 781.2259, Train Acc: 0.7779, Dev Acc: 0.6752\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "embed_size = 64\n",
    "hidden_size = 128\n",
    "num_layers = 1\n",
    "cell_type = \"LSTM\"\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize model\n",
    "encoder = Encoder(len(roman2idx), embed_size, hidden_size, num_layers, cell_type).to(device)\n",
    "decoder = Decoder(len(devanagari2idx), embed_size, hidden_size, num_layers, cell_type).to(device)\n",
    "model = Seq2Seq(encoder, decoder, cell_type).to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=devanagari2idx[\"<pad>\"])\n",
    "\n",
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "            pred = output.argmax(dim=2)\n",
    "            for i in range(tgt.size(0)):\n",
    "                for j in range(1, tgt.size(1)):\n",
    "                    if tgt[i, j].item() == devanagari2idx[\"<pad>\"]:\n",
    "                        break\n",
    "                    if pred[i, j].item() == tgt[i, j].item():\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        tgt_flat = tgt[:, 1:].reshape(-1)\n",
    "        loss = loss_function(output, tgt_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_acc = evaluate_accuracy(model, train_loader)\n",
    "    dev_acc = evaluate_accuracy(model, dev_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Train Acc: {train_acc:.4f}, Dev Acc: {dev_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d5148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4dd516c7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
