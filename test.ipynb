{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a39238",
   "metadata": {},
   "source": [
    "## Question 1 (15 Marks)\n",
    "\n",
    "Build a RNN based seq2seq model which contains the following layers: (i) input layer for character embeddings (ii) one encoder RNN which sequentially encodes the input character sequence (Latin) (iii) one decoder RNN which takes the last state of the encoder as input and produces one output character at a time (Devanagari).\n",
    "\n",
    "The code should be flexible such that the dimension of the input character embeddings, the hidden states of the encoders and decoders, the cell (RNN, LSTM, GRU) and the number of layers in the encoder and decoder can be changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33f4e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/e_disk/ch24s016/da6401_assignment3/.venv/lib/python3.10/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_vocab_size, embed_size, hidden_size, num_layers=1, cell_type=\"LSTM\"):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, embed_size)\n",
    "        rnn_class = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell_type]\n",
    "        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_vocab_size, embed_size, hidden_size, num_layers=1, cell_type=\"LSTM\"):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_vocab_size, embed_size)\n",
    "        rnn_class = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell_type]\n",
    "        self.rnn = rnn_class(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        predictions = self.fc(output.squeeze(1))  # (batch_size, vocab_size)\n",
    "        return predictions, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, cell_type=\"LSTM\"):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size, target_len = target.size()\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(device)\n",
    "        hidden = self.encoder(source)\n",
    "\n",
    "        input = target[:, 0].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = target[:, t].unsqueeze(1) if teacher_force else top1.unsqueeze(1)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30e415",
   "metadata": {},
   "source": [
    "(a) What is the total number of computations done by your network? (assume that the input embedding size is m, encoder and decoder have 1 layer each, the hidden cell state is kkk for both the encoder and decoder, the length of the input and output sequence is the same, i.e., T, the size of the vocabulary is the same for the source and target language, i.e., V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc2bb95",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d2dd19c",
   "metadata": {},
   "source": [
    "(b) What is the total number of parameters in your network? (assume that the input embedding size is M, encoder and decoder have 1 layer each, the hidden cell state is k for both the encoder and decoder and the length of the input and output sequence is the same, i.e., T, the size of the vocabulary is the same for the source and target language, i.e., V)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538d9de3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8532c72",
   "metadata": {},
   "source": [
    "## Question 2 (10 Marks)\n",
    "You will now train your model using any one language from the Dakshina dataset (I would suggest pick a language that you can read so that it is easy to analyse the errors). Use the standard train, dev, test set from the folder dakshina_dataset_v1.0/hi/lexicons/ (replace hi by the language of your choice)\n",
    "\n",
    "Using the sweep feature in wandb find the best hyperparameter configuration. Here are some suggestions but you are free to decide which hyperparameters you want to explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd08831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filepaths):\n",
    "    chars = set()\n",
    "    for filepath in filepaths:\n",
    "        with open(filepath, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                native, roman, _ = line.strip().split(\"\\t\")\n",
    "                chars.update(native)\n",
    "                chars.update(roman)\n",
    "    return chars\n",
    "\n",
    "def make_char2idx(char_set):\n",
    "    char_list = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"] + sorted(list(char_set))\n",
    "    return {char: idx for idx, char in enumerate(char_list)}, char_list\n",
    "\n",
    "train_path = \"/mnt/e_disk/ch24s016/da6401_assignment3/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.train.tsv\"\n",
    "dev_path = \"/mnt/e_disk/ch24s016/da6401_assignment3/dataset/dakshina_dataset_v1.0/ta/lexicons/ta.translit.sampled.dev.tsv\"\n",
    "\n",
    "char_set = build_vocab([train_path, dev_path])\n",
    "roman2idx, idx2roman = make_char2idx(set(c for c in char_set if c.isascii()))\n",
    "devanagari2idx, idx2devanagari = make_char2idx(set(c for c in char_set if not c.isascii()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2844381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_batch = pad_sequence(src_batch, padding_value=roman2idx[\"<pad>\"], batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=devanagari2idx[\"<pad>\"], batch_first=True)\n",
    "    return src_batch, tgt_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf0e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, tsv_path, src_char2idx, tgt_char2idx, max_len=32):\n",
    "        self.pairs = []\n",
    "        with open(tsv_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                native, roman, _ = line.strip().split('\\t')\n",
    "                self.pairs.append((roman, native))\n",
    "\n",
    "        self.src_c2i = src_char2idx\n",
    "        self.tgt_c2i = tgt_char2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        roman, native = self.pairs[i]\n",
    "\n",
    "        # map chars → indices, add <sos> / <eos> tokens as needed\n",
    "        src_idxs = [self.src_c2i.get(c, self.src_c2i[\"<unk>\"]) \n",
    "                    for c in roman][: self.max_len]\n",
    "        tgt_idxs = [self.tgt_c2i[\"<sos>\"]] + \\\n",
    "                   [self.tgt_c2i.get(c, self.tgt_c2i[\"<unk>\"]) \n",
    "                    for c in native][: (self.max_len-1)] + \\\n",
    "                   [self.tgt_c2i[\"<eos>\"]]\n",
    "\n",
    "        return torch.tensor(src_idxs), torch.tensor(tgt_idxs)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "\n",
    "    src_max_len = max(seq.size(0) for seq in src_seqs)\n",
    "    tgt_max_len = max(seq.size(0) for seq in tgt_seqs)\n",
    "\n",
    "\n",
    "    src_padded = torch.stack([\n",
    "        torch.cat([seq, torch.full((src_max_len - len(seq),), roman2idx[\"<pad>\"], dtype=torch.long)])\n",
    "        for seq in src_seqs\n",
    "    ])\n",
    "\n",
    "    tgt_padded = torch.stack([\n",
    "        torch.cat([seq, torch.full((tgt_max_len - len(seq),), devanagari2idx[\"<pad>\"], dtype=torch.long)])\n",
    "        for seq in tgt_seqs\n",
    "    ])\n",
    "\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "train_ds = TransliterationDataset(\n",
    "    \"/mnt/e_disk/ch24s016/da6401_assignment3/dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\",\n",
    "    src_char2idx=roman2idx,\n",
    "    tgt_char2idx=devanagari2idx,\n",
    "    max_len=32\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "train_dataset = TransliterationDataset(train_path, roman2idx, devanagari2idx, max_len=32)\n",
    "dev_dataset = TransliterationDataset(dev_path, roman2idx, devanagari2idx, max_len=32)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "421154af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2994.2165, Train Acc: 0.7532, Val Acc: 0.7029\n",
      "Epoch 2/10, Loss: 1085.6101, Train Acc: 0.8470, Val Acc: 0.7759\n",
      "Epoch 3/10, Loss: 786.9043, Train Acc: 0.8844, Val Acc: 0.7953\n",
      "Epoch 4/10, Loss: 632.3680, Train Acc: 0.9081, Val Acc: 0.7964\n",
      "Epoch 5/10, Loss: 531.2565, Train Acc: 0.9211, Val Acc: 0.8015\n",
      "Epoch 6/10, Loss: 452.8525, Train Acc: 0.9327, Val Acc: 0.8026\n",
      "Epoch 7/10, Loss: 401.4750, Train Acc: 0.9394, Val Acc: 0.8005\n",
      "Epoch 8/10, Loss: 354.3567, Train Acc: 0.9492, Val Acc: 0.8032\n",
      "Epoch 9/10, Loss: 320.0965, Train Acc: 0.9508, Val Acc: 0.7986\n",
      "Epoch 10/10, Loss: 291.5888, Train Acc: 0.9557, Val Acc: 0.7979\n",
      "amma → அம்மா\n",
      "vandhanam → வந்தனம்\n",
      "kaadhal → காலத்\n",
      "paattu → பாட்டு\n",
      "tamizh → தமிழ்\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "embed_size = 64\n",
    "hidden_size = 256\n",
    "num_layers = 1\n",
    "cell_type = \"LSTM\"\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "# Initialize model\n",
    "encoder = Encoder(len(roman2idx), embed_size, hidden_size, num_layers, cell_type).to(device)\n",
    "decoder = Decoder(len(devanagari2idx), embed_size, hidden_size, num_layers, cell_type).to(device)\n",
    "model = Seq2Seq(encoder, decoder, cell_type).to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=devanagari2idx[\"<pad>\"])\n",
    "\n",
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "            pred = output.argmax(dim=2)\n",
    "            for i in range(tgt.size(0)):\n",
    "                for j in range(1, tgt.size(1)):\n",
    "                    if tgt[i, j].item() == devanagari2idx[\"<pad>\"]:\n",
    "                        break\n",
    "                    if pred[i, j].item() == tgt[i, j].item():\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def predict(model, input_str, roman2idx, idx2devanagari, max_len=32):\n",
    "    model.eval()\n",
    "\n",
    "    # Convert input string to index tensor\n",
    "    input_idxs = [roman2idx.get(c, roman2idx[\"<unk>\"]) for c in input_str]\n",
    "    input_tensor = torch.tensor(input_idxs, dtype=torch.long).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = model.encoder(input_tensor)\n",
    "\n",
    "        # Start with <sos>\n",
    "        input_dec = torch.tensor([[devanagari2idx[\"<sos>\"]]], dtype=torch.long).to(device)\n",
    "\n",
    "        output_tokens = []\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = model.decoder(input_dec, hidden)\n",
    "            top1 = output.argmax(1).item()\n",
    "\n",
    "            if top1 == devanagari2idx[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "            output_tokens.append(top1)\n",
    "            input_dec = torch.tensor([[top1]], dtype=torch.long).to(device)\n",
    "\n",
    "    # Convert indices back to characters\n",
    "    return ''.join([idx2devanagari[i] for i in output_tokens])\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        tgt_flat = tgt[:, 1:].reshape(-1)\n",
    "        loss = loss_function(output, tgt_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    train_acc = evaluate_accuracy(model, train_loader)\n",
    "    dev_acc = evaluate_accuracy(model, dev_loader)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {dev_acc:.4f}\")\n",
    "    \n",
    "test_words = [\"amma\", \"vandhanam\", \"kaadhal\", \"paattu\", \"tamizh\"]\n",
    "\n",
    "for word in test_words:\n",
    "    output = predict(model, word, roman2idx, idx2devanagari)\n",
    "    print(f\"{word} → {output}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c85abdf",
   "metadata": {},
   "source": [
    "## Wandb Sweep Run to Find Best Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d5148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: gy0rcmqo\n",
      "Sweep URL: https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/sweeps/gy0rcmqo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tidwg9cp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: RNN\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mch24s016\u001b[0m (\u001b[33mch24s016-iitm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/ch24s016/da6401_assignment3/wandb/run-20250520_001751-tidwg9cp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/runs/tidwg9cp' target=\"_blank\">whole-sweep-1</a></strong> to <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/sweeps/gy0rcmqo' target=\"_blank\">https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/sweeps/gy0rcmqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3' target=\"_blank\">https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/sweeps/gy0rcmqo' target=\"_blank\">https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/sweeps/gy0rcmqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/runs/tidwg9cp' target=\"_blank\">https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/runs/tidwg9cp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'Seq2Seq Transliteration Sweep',\n",
    "    'metric': {'name': \"val_accuracy\", 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embed_size': {'values': [32, 64, 128]},\n",
    "        'hidden_size': {'values': [64, 128, 256]},\n",
    "        'num_layers': {'values': [1]},\n",
    "        'cell_type': {'values': ['RNN', 'GRU', 'LSTM']},\n",
    "        'optimizer': {'values': ['adam', 'adamw', 'sgd']},\n",
    "        'lr': {'values': [0.01, 0.001, 0.0005]},\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [5, 10]}\n",
    "    },\n",
    "}\n",
    "\n",
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "            pred = output.argmax(dim=2)\n",
    "            for i in range(tgt.size(0)):\n",
    "                for j in range(1, tgt.size(1)):\n",
    "                    if tgt[i, j].item() == devanagari2idx[\"<pad>\"]:\n",
    "                        break\n",
    "                    if pred[i, j].item() == tgt[i, j].item():\n",
    "                        correct += 1\n",
    "                    total += 1\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "def train_sweep():\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    # Update hyperparameters from sweep config\n",
    "    embed_size = config.embed_size\n",
    "    hidden_size = config.hidden_size\n",
    "    num_layers = config.num_layers\n",
    "    cell_type = config.cell_type\n",
    "    batch_size = config.batch_size\n",
    "    epochs = config.epochs\n",
    "    lr = config.lr\n",
    "\n",
    "    # Update data loader if batch_size changes\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # Model setup\n",
    "    encoder = Encoder(len(roman2idx), embed_size, hidden_size, num_layers, cell_type).to(device)\n",
    "    decoder = Decoder(len(devanagari2idx), embed_size, hidden_size, num_layers, cell_type).to(device)\n",
    "    model = Seq2Seq(encoder, decoder, cell_type).to(device)\n",
    "\n",
    "    # Optimizer\n",
    "    if config.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif config.optimizer == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    # \n",
    "    # elif config.optimizer == 'sgd':\n",
    "    #     optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    # elif config.optimizer == 'rmsprop':\n",
    "    #     optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    # Loss function\n",
    "    loss_function = nn.CrossEntropyLoss(ignore_index=devanagari2idx[\"<pad>\"])\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for src, tgt in train_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            tgt_flat = tgt[:, 1:].reshape(-1)\n",
    "            loss = loss_function(output, tgt_flat)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_acc = evaluate_accuracy(model, train_loader)\n",
    "        val_acc = evaluate_accuracy(model, dev_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"loss\": total_loss,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "        \n",
    "    model_dir = \"./trained_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Unique file name using wandb run name or ID\n",
    "    run_id = wandb.run.name  # or wandb.run.id\n",
    "    model_path = os.path.join(model_dir, f\"model_{run_id}.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Seq2SeqAssignment3\")\n",
    "wandb.agent(sweep_id, function=train_sweep, count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bab80556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bestsweepsofar</strong> at: <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/runs/8uk2tyzb' target=\"_blank\">https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/runs/8uk2tyzb</a><br> View project at: <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3' target=\"_blank\">https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_013012-8uk2tyzb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/ch24s016/da6401_assignment3/wandb/run-20250520_013035-myhkj85n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch24s016-iitm/da6401_assignment3/runs/myhkj85n' target=\"_blank\">sage-firebrand-7</a></strong> to <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment3' target=\"_blank\">https://wandb.ai/ch24s016-iitm/da6401_assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment3/runs/myhkj85n' target=\"_blank\">https://wandb.ai/ch24s016-iitm/da6401_assignment3/runs/myhkj85n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage-firebrand-7</strong> at: <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment3/runs/myhkj85n' target=\"_blank\">https://wandb.ai/ch24s016-iitm/da6401_assignment3/runs/myhkj85n</a><br> View project at: <a href='https://wandb.ai/ch24s016-iitm/da6401_assignment3' target=\"_blank\">https://wandb.ai/ch24s016-iitm/da6401_assignment3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_013035-myhkj85n/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e_disk/ch24s016/da6401_assignment3/wandb/run-20250520_013038-isip1mu0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/runs/isip1mu0' target=\"_blank\">bestsweepsofar</a></strong> to <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3' target=\"_blank\">https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/runs/isip1mu0' target=\"_blank\">https://wandb.ai/ch24s016-iitm/Seq2SeqAssignment3/runs/isip1mu0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.0005, 'epochs': 10, 'cell_type': 'LSTM', 'optimizer': 'adamw', 'batch_size': 32, 'embed_size': 32, 'num_layers': 1, 'hidden_size': 128}\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "wandb.init()\n",
    "wandb.init(project=\"Seq2SeqAssignment3\",entity=\"ch24s016-iitm\",name='bestsweepsofar')\n",
    "api = wandb.Api()\n",
    "\n",
    "# Fetch all runs in the sweep\n",
    "sweep_runs = api.sweep(f\"ch24s016-iitm/Seq2SeqAssignment3/gy0rcmqo/\").runs\n",
    "\n",
    "\n",
    "# Find the best model based on validation accuracy\n",
    "best_run = max(sweep_runs, key=lambda run: run.summary.get(\"val_accuracy\", 0))\n",
    "\n",
    "print(best_run.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd516c7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
